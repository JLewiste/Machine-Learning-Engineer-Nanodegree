{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report: Project Smartcab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement a Basic Driving Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION: Observe what you see with the agent's behavior as it takes random actions. Does the smartcab eventually make it to the destination? Are there any other interesting observations to note?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "According to my obersvation, when the agent is set to take random actions, it eventually makes it to the destination but rarely. The approach to reach the goal is clearly random and not optimal. As expected the agent moves randomly across the map, ignoring rewards and penalties during its course of actions. \n",
    "\n",
    "It keeps on moving randomly without any knowledge of the past. Apart from that, it does not exploit what it has already discover in order to obtain reward (it does not know that running a red-light will produce penalties)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inform the Driving Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION: What states have you identified that are appropriate for modeling the smartcab and environment? Why do you believe each of these states to be appropriate for this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "According to my analysis, the states that are appropriate for modeling the smartcab and environment are:\n",
    "    1. Next waypoint\n",
    "    2. Status of traffic lights (green and red)\n",
    "    3. Status of traffic at that intersection\n",
    "    \n",
    "In my opinion, the result of next waypoint is crucial for modeling the smartcab and environment. This information is used to define the target to reach the destination. Next waypoint also allow the agent to reach its destination efficiently.\n",
    "\n",
    "Apart from that, status of traffic ligts (green and red) ensures that our agent obeys the traffic rules while reaching its destination. \n",
    "\n",
    "Next, status of traffic light at that intersection helps in reducing penalties and increasing our cumulative success rate.\n",
    "\n",
    "Lastly, there is another state that is missing out which is deadline. By adding deadline into the list of states, performance of the agent degraded. The sudden drop in performace is caused by Q matrix being too sparse. This will cause the prediction step to fail to find the required value in Q matrix (fail to converge). In order for the Q matrix to converge, we need more than 100 trials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement a Q-Learning Driving Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION: What changes do you notice in the agent's behavior when compared to the basic driving agent when random actions were always taken? Why is this behavior occurring?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "According to my analysis, once the Q-Learnning has been implemented the agent's behavior has been improved dramatically when compared to the basic driving agent(when random actions were always taken).\n",
    "\n",
    "Q-Learning gives the agent the ability to obey traffic lights rules while reaching its destination. The agent does not sit idle when the traffic light is green. On the other hand, it does not choose the red light direction once the agent knows the bad impact of this choice.\n",
    "\n",
    "This behavior is occuring because each action take by the agent will produce a reward which depends on the state of the environment. The agent will try its best to minize the penalties in order to reach its destination in the most efficient manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improve the Q-Learning Driving Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION: Report the different values for the parameters tuned in your basic implementation of Q-Learning. For which set of parameters does the agent perform best? How well does the final driving agent perform?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"One.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Run: When the Q-Learning paramaters are set to the following:\n",
    "    - Alpha(Learning Rate) = 0.1\n",
    "    - Gamma(Discount Rate) = 0.1\n",
    "    - Epsilon(Stimulated Annealing) = 5\n",
    "    - Smartcab's success rate is approximately 90%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Two.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second Run: When the Q-Learning paramaters are set to the following:\n",
    "    - Alpha(Learning Rate) = 0.5\n",
    "    - Gamma(Discount Rate) = 0.5\n",
    "    - Epsilon(Stimulated Annealing) = 10\n",
    "    - Smartcab's success rate is approximately 81%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Three.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third Run: When the Q-Learning paramaters are set to the following:\n",
    "    - Alpha(Learning Rate) = 0.8\n",
    "    - Gamma(Discount Rate) = 0.2\n",
    "    - Epsilon(Stimulated Annealing) = 4\n",
    "    - Smartcab's success rate is approximately 99%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to my analysis, the agent performs best during its third run where Alpha=0.8, Gamma=0.2 and Epsilon=4. The final driving agent is able to complete 98 out of 100 trials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION: Does your agent get close to finding an optimal policy, i.e. reach the destination in the minimum possible time, and not incur any penalties? How would you describe an optimal policy for this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "According to my analysis, the agent does get close to finding an optimal policy, ie. reaching the destination in the shortest possible time while minimising penalties.\n",
    "\n",
    "The optimal policy for Smartcab problem would be:\n",
    "    1. The agent accumulates as much rewards as possible while still reaching its destination in time.\n",
    "    2. The agent minimises as much penalties as possible while still reaching its destination in time.\n",
    "    3. The agent is able to obey all traffic rules.\n",
    "    4. The agent is able to avoid colliding with other cars.\n",
    "\n",
    "Apart from that, I noticed that there is 1 issue regarding the rewards/penalties system for the Smartcab:\n",
    "    1. The agent will tend to disobey the traffic when the time is running out.\n",
    "        - The agent might not remain stationary even when the traffic is red.\n",
    "        - This would cause accidents in real life."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
