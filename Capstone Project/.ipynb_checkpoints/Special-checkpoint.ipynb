{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/firdause/Downloads/Special/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting /Users/firdause/Downloads/Special/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting /Users/firdause/Downloads/Special/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /Users/firdause/Downloads/Special/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist_data = input_data.read_data_sets('/Users/firdause/Downloads/Special/MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = mnist_data.train.images\n",
    "y_train = mnist_data.train.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = mnist_data.test.images\n",
    "y_test = mnist_data.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the next 64 images array and labels\n",
    "batch_X, batch_Y = mnist_data.train.next_batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "greetings = tf.constant('Hey people!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey people!\n"
     ]
    }
   ],
   "source": [
    "print sess.run(greetings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Basic constant operations\n",
    "# The value returned by the constructor represents the output\n",
    "# of the Constant op.\n",
    "a = tf.constant(2)\n",
    "b = tf.constant(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=2, b=3\n",
      "Addition with constants: 5\n",
      "Multiplication with constants: 6\n"
     ]
    }
   ],
   "source": [
    "# Launch the default graph.\n",
    "with tf.Session() as sess:\n",
    "    print \"a=2, b=3\"\n",
    "    print \"Addition with constants: %i\" % sess.run(a+b)\n",
    "    print \"Multiplication with constants: %i\" % sess.run(a*b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Basic Operations with variable as graph input\n",
    "# The value returned by the constructor represents the output\n",
    "# of the Variable op. (define as input when running session)\n",
    "# tf Graph input\n",
    "a = tf.placeholder(tf.int16)\n",
    "b = tf.placeholder(tf.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define some operations\n",
    "addition_ops = tf.add(a, b)\n",
    "multiplication_ops = tf.mul(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition with variables: 5\n",
      "Multiplication with variables: 6\n"
     ]
    }
   ],
   "source": [
    "# Launch the default graph.\n",
    "with tf.Session() as sess:\n",
    "    # Run every operation with variable input\n",
    "    print \"Addition with variables: %i\" % sess.run(addition_ops, feed_dict={a: 2, b: 3})\n",
    "    print \"Multiplication with variables: %i\" % sess.run(multiplication_ops, feed_dict={a: 2, b: 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------\n",
    "# More in details:\n",
    "# Matrix Multiplication from TensorFlow official tutorial\n",
    "\n",
    "# Create a Constant op that produces a 1x2 matrix.  The op is\n",
    "# added as a node to the default graph.\n",
    "#\n",
    "# The value returned by the constructor represents the output\n",
    "# of the Constant op.\n",
    "matrix1 = tf.constant([[3., 3.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create another Constant that produces a 2x1 matrix.\n",
    "matrix2 = tf.constant([[2.],[2.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a Matmul op that takes 'matrix1' and 'matrix2' as inputs.\n",
    "# The returned value, 'product', represents the result of the matrix\n",
    "# multiplication.\n",
    "product = tf.matmul(matrix1, matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 12.]]\n"
     ]
    }
   ],
   "source": [
    "# To run the matmul op we call the session 'run()' method, passing 'product'\n",
    "# which represents the output of the matmul op.  This indicates to the call\n",
    "# that we want to get the output of the matmul op back.\n",
    "#\n",
    "# All inputs needed by the op are run automatically by the session.  They\n",
    "# typically are run in parallel.\n",
    "#\n",
    "# The call 'run(product)' thus causes the execution of threes ops in the\n",
    "# graph: the two constants and matmul.\n",
    "#\n",
    "# The output of the op is returned in 'result' as a numpy `ndarray` object.\n",
    "with tf.Session() as sess:\n",
    "    result = sess.run(product)\n",
    "    print result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/firdause/Downloads/Special/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting /Users/firdause/Downloads/Special/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting /Users/firdause/Downloads/Special/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /Users/firdause/Downloads/Special/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('/Users/firdause/Downloads/Special/MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In this example, we limit mnist data\n",
    "X_train, y_train = mnist.train.next_batch(5000) #5000 for training (nn candidates)\n",
    "X_test, y_test = mnist.test.next_batch(100) #1000 for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf Graph Input\n",
    "xtr = tf.placeholder(\"float\", [None, 784])\n",
    "xte = tf.placeholder(\"float\", [784])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Nearest Neighbor calculation using L1 Distance\n",
    "# Calculate L1 Distance\n",
    "distance = tf.reduce_sum(tf.abs(tf.add(xtr, tf.neg(xte))), reduction_indices=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prediction: Get min distance index (Nearest neighbor)\n",
    "pred = tf.arg_min(distance, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy = 0.\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 0 Prediction: 9 True Class: 9\n",
      "Test 1 Prediction: 0 True Class: 0\n",
      "Test 2 Prediction: 3 True Class: 2\n",
      "Test 3 Prediction: 5 True Class: 5\n",
      "Test 4 Prediction: 1 True Class: 1\n",
      "Test 5 Prediction: 9 True Class: 9\n",
      "Test 6 Prediction: 7 True Class: 7\n",
      "Test 7 Prediction: 8 True Class: 8\n",
      "Test 8 Prediction: 1 True Class: 1\n",
      "Test 9 Prediction: 0 True Class: 0\n",
      "Test 10 Prediction: 4 True Class: 4\n",
      "Test 11 Prediction: 1 True Class: 1\n",
      "Test 12 Prediction: 7 True Class: 7\n",
      "Test 13 Prediction: 9 True Class: 9\n",
      "Test 14 Prediction: 2 True Class: 6\n",
      "Test 15 Prediction: 4 True Class: 4\n",
      "Test 16 Prediction: 2 True Class: 2\n",
      "Test 17 Prediction: 6 True Class: 6\n",
      "Test 18 Prediction: 8 True Class: 8\n",
      "Test 19 Prediction: 1 True Class: 1\n",
      "Test 20 Prediction: 3 True Class: 3\n",
      "Test 21 Prediction: 7 True Class: 7\n",
      "Test 22 Prediction: 5 True Class: 5\n",
      "Test 23 Prediction: 4 True Class: 4\n",
      "Test 24 Prediction: 4 True Class: 4\n",
      "Test 25 Prediction: 1 True Class: 1\n",
      "Test 26 Prediction: 8 True Class: 8\n",
      "Test 27 Prediction: 1 True Class: 1\n",
      "Test 28 Prediction: 3 True Class: 3\n",
      "Test 29 Prediction: 8 True Class: 8\n",
      "Test 30 Prediction: 1 True Class: 1\n",
      "Test 31 Prediction: 2 True Class: 2\n",
      "Test 32 Prediction: 4 True Class: 5\n",
      "Test 33 Prediction: 8 True Class: 8\n",
      "Test 34 Prediction: 0 True Class: 0\n",
      "Test 35 Prediction: 6 True Class: 6\n",
      "Test 36 Prediction: 2 True Class: 2\n",
      "Test 37 Prediction: 1 True Class: 1\n",
      "Test 38 Prediction: 1 True Class: 1\n",
      "Test 39 Prediction: 1 True Class: 7\n",
      "Test 40 Prediction: 1 True Class: 1\n",
      "Test 41 Prediction: 5 True Class: 5\n",
      "Test 42 Prediction: 3 True Class: 3\n",
      "Test 43 Prediction: 4 True Class: 4\n",
      "Test 44 Prediction: 6 True Class: 6\n",
      "Test 45 Prediction: 9 True Class: 9\n",
      "Test 46 Prediction: 5 True Class: 5\n",
      "Test 47 Prediction: 0 True Class: 0\n",
      "Test 48 Prediction: 9 True Class: 9\n",
      "Test 49 Prediction: 2 True Class: 2\n",
      "Test 50 Prediction: 3 True Class: 2\n",
      "Test 51 Prediction: 4 True Class: 4\n",
      "Test 52 Prediction: 8 True Class: 8\n",
      "Test 53 Prediction: 2 True Class: 2\n",
      "Test 54 Prediction: 1 True Class: 1\n",
      "Test 55 Prediction: 7 True Class: 7\n",
      "Test 56 Prediction: 2 True Class: 2\n",
      "Test 57 Prediction: 4 True Class: 4\n",
      "Test 58 Prediction: 9 True Class: 9\n",
      "Test 59 Prediction: 4 True Class: 4\n",
      "Test 60 Prediction: 4 True Class: 4\n",
      "Test 61 Prediction: 0 True Class: 0\n",
      "Test 62 Prediction: 3 True Class: 3\n",
      "Test 63 Prediction: 9 True Class: 9\n",
      "Test 64 Prediction: 2 True Class: 2\n",
      "Test 65 Prediction: 2 True Class: 2\n",
      "Test 66 Prediction: 3 True Class: 3\n",
      "Test 67 Prediction: 3 True Class: 3\n",
      "Test 68 Prediction: 4 True Class: 8\n",
      "Test 69 Prediction: 3 True Class: 3\n",
      "Test 70 Prediction: 5 True Class: 5\n",
      "Test 71 Prediction: 7 True Class: 7\n",
      "Test 72 Prediction: 3 True Class: 3\n",
      "Test 73 Prediction: 5 True Class: 5\n",
      "Test 74 Prediction: 8 True Class: 8\n",
      "Test 75 Prediction: 1 True Class: 1\n",
      "Test 76 Prediction: 2 True Class: 2\n",
      "Test 77 Prediction: 9 True Class: 4\n",
      "Test 78 Prediction: 4 True Class: 4\n",
      "Test 79 Prediction: 6 True Class: 6\n",
      "Test 80 Prediction: 4 True Class: 4\n",
      "Test 81 Prediction: 9 True Class: 9\n",
      "Test 82 Prediction: 3 True Class: 5\n",
      "Test 83 Prediction: 1 True Class: 1\n",
      "Test 84 Prediction: 0 True Class: 0\n",
      "Test 85 Prediction: 6 True Class: 6\n",
      "Test 86 Prediction: 9 True Class: 9\n",
      "Test 87 Prediction: 5 True Class: 5\n",
      "Test 88 Prediction: 9 True Class: 9\n",
      "Test 89 Prediction: 5 True Class: 5\n",
      "Test 90 Prediction: 9 True Class: 9\n",
      "Test 91 Prediction: 7 True Class: 7\n",
      "Test 92 Prediction: 3 True Class: 3\n",
      "Test 93 Prediction: 8 True Class: 8\n",
      "Test 94 Prediction: 0 True Class: 0\n",
      "Test 95 Prediction: 3 True Class: 3\n",
      "Test 96 Prediction: 4 True Class: 7\n",
      "Test 97 Prediction: 1 True Class: 1\n",
      "Test 98 Prediction: 3 True Class: 3\n",
      "Test 99 Prediction: 6 True Class: 6\n",
      "Done!\n",
      "Accuracy: 0.91\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # loop over test data\n",
    "    for i in range(len(X_test)):\n",
    "        # Get nearest neighbor\n",
    "        nn_index = sess.run(pred, feed_dict={xtr: X_train, xte: X_test[i, :]})\n",
    "        # Get nearest neighbor class label and compare it to its true label\n",
    "        print 'Test', i, 'Prediction:', np.argmax(y_train[nn_index]), 'True Class:', np.argmax(y_test[i])\n",
    "        # Calculate accuracy\n",
    "        if np.argmax(y_train[nn_index]) == np.argmax(y_test[i]):\n",
    "            accuracy += 1./len(X_test)\n",
    "            \n",
    "    print \"Done!\"\n",
    "    print \"Accuracy:\", accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "rng = numpy.random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 1000\n",
    "display_step = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training Data\n",
    "train_X = numpy.asarray([3.3, 4.4, 5.5, 6.71, 6.93, 4.168, 9.779, 6.182, 7.59, 2.167, 7.042, 10.791, 5.313, 7.997, 5.654, 9.27, 3.1])\n",
    "train_Y = numpy.asarray([1.7, 2.76, 2.09, 3.19, 1.694, 1.573, 3.366, 2.596, 2.53, 1.221, 2.827, 3.465, 1.65, 2.904, 2.42, 2.94, 1.3])\n",
    "n_samples = train_X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf Graph Input\n",
    "X = tf.placeholder(\"float\")\n",
    "Y = tf.placeholder(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set model weights\n",
    "W = tf.Variable(rng.randn(), name=\"weight\")\n",
    "b = tf.Variable(rng.randn(), name=\"bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct a linear model\n",
    "pred = tf.add(tf.mul(X, W), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mean squared error\n",
    "cost = tf.reduce_sum(tf.pow(pred-Y, 2))/(2*n_samples)\n",
    "\n",
    "# Gradient descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0050 cost= 0.098212801 W= 0.331181 b= 0.214529\n",
      "Epoch: 0100 cost= 0.095756695 W= 0.326329 b= 0.249437\n",
      "Epoch: 0150 cost= 0.093584463 W= 0.321765 b= 0.282269\n",
      "Epoch: 0200 cost= 0.091663279 W= 0.317473 b= 0.313149\n",
      "Epoch: 0250 cost= 0.089964248 W= 0.313436 b= 0.342191\n",
      "Epoch: 0300 cost= 0.088461623 W= 0.309639 b= 0.369506\n",
      "Epoch: 0350 cost= 0.087132692 W= 0.306067 b= 0.395198\n",
      "Epoch: 0400 cost= 0.085957520 W= 0.302709 b= 0.41936\n",
      "Epoch: 0450 cost= 0.084918208 W= 0.29955 b= 0.442085\n",
      "Epoch: 0500 cost= 0.083999105 W= 0.296579 b= 0.46346\n",
      "Epoch: 0550 cost= 0.083186299 W= 0.293784 b= 0.483563\n",
      "Epoch: 0600 cost= 0.082467586 W= 0.291156 b= 0.50247\n",
      "Epoch: 0650 cost= 0.081832021 W= 0.288684 b= 0.520253\n",
      "Epoch: 0700 cost= 0.081270039 W= 0.286359 b= 0.536978\n",
      "Epoch: 0750 cost= 0.080773070 W= 0.284172 b= 0.552708\n",
      "Epoch: 0800 cost= 0.080333658 W= 0.282116 b= 0.567504\n",
      "Epoch: 0850 cost= 0.079945110 W= 0.280182 b= 0.581419\n",
      "Epoch: 0900 cost= 0.079601564 W= 0.278362 b= 0.594507\n",
      "Epoch: 0950 cost= 0.079297811 W= 0.276651 b= 0.606818\n",
      "Epoch: 1000 cost= 0.079029292 W= 0.275042 b= 0.618394\n",
      "Optimization Finished!\n",
      "Training cost= 0.0790293 W= 0.275042 b= 0.618394 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgkAAAFkCAYAAACq4KjhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10504a110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Fit all training data\n",
    "    for epoch in range(training_epochs):\n",
    "        for (x, y) in zip(train_X, train_Y):\n",
    "            sess.run(optimizer, feed_dict={X: x, Y: y})\n",
    "            \n",
    "        #Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            c = sess.run(cost, feed_dict={X: train_X, Y:train_Y})\n",
    "            print 'Epoch:', '%04d' % (epoch+1), 'cost=', '{:.9f}'.format(c), 'W=', sess.run(W), 'b=', sess.run(b)\n",
    "            \n",
    "    print \"Optimization Finished!\"\n",
    "    training_cost = sess.run(cost, feed_dict={X: train_X, Y: train_Y})\n",
    "    print \"Training cost=\", training_cost, \"W=\", sess.run(W), \"b=\", sess.run(b), '\\n'\n",
    "    \n",
    "    \n",
    "    #Graphic display\n",
    "    plt.plot(train_X, train_Y, 'ro', label='Original data')\n",
    "    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/firdause/Downloads/Special/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting /Users/firdause/Downloads/Special/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting /Users/firdause/Downloads/Special/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /Users/firdause/Downloads/Special/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('/Users/firdause/Downloads/Special/MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 25\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# tf Graph Input\n",
    "x = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784\n",
    "y = tf.placeholder(tf.float32, [None, 10]) # 0-9 digits recognition => 10 classes\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# Construct model\n",
    "pred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n",
    "\n",
    "# Minimize error using cross entropy\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "# Gradient Descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 1.182138980\n",
      "Epoch: 0002 cost= 0.664748238\n",
      "Epoch: 0003 cost= 0.552650713\n",
      "Epoch: 0004 cost= 0.498593372\n",
      "Epoch: 0005 cost= 0.465485760\n",
      "Epoch: 0006 cost= 0.442463916\n",
      "Epoch: 0007 cost= 0.425566151\n",
      "Epoch: 0008 cost= 0.412155169\n",
      "Epoch: 0009 cost= 0.401349639\n",
      "Epoch: 0010 cost= 0.392361337\n",
      "Epoch: 0011 cost= 0.384718975\n",
      "Epoch: 0012 cost= 0.378173611\n",
      "Epoch: 0013 cost= 0.372397706\n",
      "Epoch: 0014 cost= 0.367291180\n",
      "Epoch: 0015 cost= 0.362717236\n",
      "Epoch: 0016 cost= 0.358618587\n",
      "Epoch: 0017 cost= 0.354868329\n",
      "Epoch: 0018 cost= 0.351401907\n",
      "Epoch: 0019 cost= 0.348290493\n",
      "Epoch: 0020 cost= 0.345436420\n",
      "Epoch: 0021 cost= 0.342688507\n",
      "Epoch: 0022 cost= 0.340233569\n",
      "Epoch: 0023 cost= 0.337940860\n",
      "Epoch: 0024 cost= 0.335738704\n",
      "Epoch: 0025 cost= 0.333623322\n",
      "Optimization Finished!\n",
      "Accuracy: 0.888667\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # Fit training using batch data\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs,\n",
    "                                                          y: batch_ys})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print \"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost)\n",
    "\n",
    "    print \"Optimization Finished!\"\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy for 3000 examples\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print \"Accuracy:\", accuracy.eval({x: mnist.test.images[:3000], y: mnist.test.labels[:3000]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/firdause/Downloads/Special/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting /Users/firdause/Downloads/Special/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting /Users/firdause/Downloads/Special/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /Users/firdause/Downloads/Special/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('/Users/firdause/Downloads/Special/MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 100\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of features\n",
    "n_hidden_2 = 256 # 2nd layer number of features\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 156.542353727\n",
      "Epoch: 0002 cost= 40.132511560\n",
      "Epoch: 0003 cost= 25.242863284\n",
      "Epoch: 0004 cost= 17.411551378\n",
      "Epoch: 0005 cost= 12.417694036\n",
      "Epoch: 0006 cost= 9.269824356\n",
      "Epoch: 0007 cost= 6.881440638\n",
      "Epoch: 0008 cost= 5.130062710\n",
      "Epoch: 0009 cost= 3.773871822\n",
      "Epoch: 0010 cost= 2.883989847\n",
      "Epoch: 0011 cost= 2.102231810\n",
      "Epoch: 0012 cost= 1.541434126\n",
      "Epoch: 0013 cost= 1.204768012\n",
      "Epoch: 0014 cost= 0.959595424\n",
      "Epoch: 0015 cost= 0.841861494\n",
      "Epoch: 0016 cost= 0.703358863\n",
      "Epoch: 0017 cost= 0.546156618\n",
      "Epoch: 0018 cost= 0.507498001\n",
      "Epoch: 0019 cost= 0.457615834\n",
      "Epoch: 0020 cost= 0.360885320\n",
      "Epoch: 0021 cost= 0.383696878\n",
      "Epoch: 0022 cost= 0.287874705\n",
      "Epoch: 0023 cost= 0.304861618\n",
      "Epoch: 0024 cost= 0.346580610\n",
      "Epoch: 0025 cost= 0.302558155\n",
      "Epoch: 0026 cost= 0.337058896\n",
      "Epoch: 0027 cost= 0.253081449\n",
      "Epoch: 0028 cost= 0.342187149\n",
      "Epoch: 0029 cost= 0.219832295\n",
      "Epoch: 0030 cost= 0.223422835\n",
      "Epoch: 0031 cost= 0.215357968\n",
      "Epoch: 0032 cost= 0.255601718\n",
      "Epoch: 0033 cost= 0.218980473\n",
      "Epoch: 0034 cost= 0.181531797\n",
      "Epoch: 0035 cost= 0.230024821\n",
      "Epoch: 0036 cost= 0.238102139\n",
      "Epoch: 0037 cost= 0.161201598\n",
      "Epoch: 0038 cost= 0.202557698\n",
      "Epoch: 0039 cost= 0.238419952\n",
      "Epoch: 0040 cost= 0.202182068\n",
      "Epoch: 0041 cost= 0.196296798\n",
      "Epoch: 0042 cost= 0.191706887\n",
      "Epoch: 0043 cost= 0.243707354\n",
      "Epoch: 0044 cost= 0.156867721\n",
      "Epoch: 0045 cost= 0.190544756\n",
      "Epoch: 0046 cost= 0.135088546\n",
      "Epoch: 0047 cost= 0.158834951\n",
      "Epoch: 0048 cost= 0.155608654\n",
      "Epoch: 0049 cost= 0.208547254\n",
      "Epoch: 0050 cost= 0.181985060\n",
      "Epoch: 0051 cost= 0.144339339\n",
      "Epoch: 0052 cost= 0.143960875\n",
      "Epoch: 0053 cost= 0.070256385\n",
      "Epoch: 0054 cost= 0.124791447\n",
      "Epoch: 0055 cost= 0.178060517\n",
      "Epoch: 0056 cost= 0.111171792\n",
      "Epoch: 0057 cost= 0.164460248\n",
      "Epoch: 0058 cost= 0.163468439\n",
      "Epoch: 0059 cost= 0.099697253\n",
      "Epoch: 0060 cost= 0.107840303\n",
      "Epoch: 0061 cost= 0.173575991\n",
      "Epoch: 0062 cost= 0.142382100\n",
      "Epoch: 0063 cost= 0.155034773\n",
      "Epoch: 0064 cost= 0.147290600\n",
      "Epoch: 0065 cost= 0.134068358\n",
      "Epoch: 0066 cost= 0.087043963\n",
      "Epoch: 0067 cost= 0.137575722\n",
      "Epoch: 0068 cost= 0.123282383\n",
      "Epoch: 0069 cost= 0.145302634\n",
      "Epoch: 0070 cost= 0.128047163\n",
      "Epoch: 0071 cost= 0.070123675\n",
      "Epoch: 0072 cost= 0.083890181\n",
      "Epoch: 0073 cost= 0.098247398\n",
      "Epoch: 0074 cost= 0.099640173\n",
      "Epoch: 0075 cost= 0.112963749\n",
      "Epoch: 0076 cost= 0.113042264\n",
      "Epoch: 0077 cost= 0.100165373\n",
      "Epoch: 0078 cost= 0.142333040\n",
      "Epoch: 0079 cost= 0.121704263\n",
      "Epoch: 0080 cost= 0.088370372\n",
      "Epoch: 0081 cost= 0.144334944\n",
      "Epoch: 0082 cost= 0.120774513\n",
      "Epoch: 0083 cost= 0.069155193\n",
      "Epoch: 0084 cost= 0.080618112\n",
      "Epoch: 0085 cost= 0.095796792\n",
      "Epoch: 0086 cost= 0.098191255\n",
      "Epoch: 0087 cost= 0.103373761\n",
      "Epoch: 0088 cost= 0.116695851\n",
      "Epoch: 0089 cost= 0.132698464\n",
      "Epoch: 0090 cost= 0.058177342\n",
      "Epoch: 0091 cost= 0.107600395\n",
      "Epoch: 0092 cost= 0.105370606\n",
      "Epoch: 0093 cost= 0.091124870\n",
      "Epoch: 0094 cost= 0.083295983\n",
      "Epoch: 0095 cost= 0.086391927\n",
      "Epoch: 0096 cost= 0.061689236\n",
      "Epoch: 0097 cost= 0.111840998\n",
      "Epoch: 0098 cost= 0.094415079\n",
      "Epoch: 0099 cost= 0.073856313\n",
      "Epoch: 0100 cost= 0.079020798\n",
      "Optimization Finished!\n",
      "Accuracy: 0.968\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x, y: batch_y})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print \"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost)\n",
    "    print \"Optimization Finished!\"\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print \"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part 7: CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/firdause/Downloads/Special/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting /Users/firdause/Downloads/Special/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting /Users/firdause/Downloads/Special/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /Users/firdause/Downloads/Special/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('/Users/firdause/Downloads/Special/MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 200000\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "dropout = 0.75 # Dropout, probability to keep units\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create some wrappers for simplicity\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n",
    "\n",
    "\n",
    "# Create model\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # Reshape input picture\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Apply Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    # 5x5 conv, 1 input, 32 outputs\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "    # 5x5 conv, 32 inputs, 64 outputs\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
    "    # 1024 inputs, 10 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.random_normal([1024, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = conv_net(x, weights, biases, keep_prob)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1280, Minibatch Loss= 25755.042969, Training Accuracy= 0.27344\n",
      "Iter 2560, Minibatch Loss= 12072.064453, Training Accuracy= 0.50000\n",
      "Iter 3840, Minibatch Loss= 11676.448242, Training Accuracy= 0.55469\n",
      "Iter 5120, Minibatch Loss= 5224.934570, Training Accuracy= 0.70312\n",
      "Iter 6400, Minibatch Loss= 3957.132812, Training Accuracy= 0.77344\n",
      "Iter 7680, Minibatch Loss= 8581.132812, Training Accuracy= 0.72656\n",
      "Iter 8960, Minibatch Loss= 3634.357910, Training Accuracy= 0.76562\n",
      "Iter 10240, Minibatch Loss= 3968.686523, Training Accuracy= 0.77344\n",
      "Iter 11520, Minibatch Loss= 2676.420410, Training Accuracy= 0.83594\n",
      "Iter 12800, Minibatch Loss= 3675.953857, Training Accuracy= 0.79688\n",
      "Iter 14080, Minibatch Loss= 1508.251221, Training Accuracy= 0.87500\n",
      "Iter 15360, Minibatch Loss= 1760.329590, Training Accuracy= 0.90625\n",
      "Iter 16640, Minibatch Loss= 2529.463379, Training Accuracy= 0.86719\n",
      "Iter 17920, Minibatch Loss= 1811.808350, Training Accuracy= 0.88281\n",
      "Iter 19200, Minibatch Loss= 1766.270264, Training Accuracy= 0.87500\n",
      "Iter 20480, Minibatch Loss= 533.533630, Training Accuracy= 0.95312\n",
      "Iter 21760, Minibatch Loss= 3216.217773, Training Accuracy= 0.84375\n",
      "Iter 23040, Minibatch Loss= 964.133850, Training Accuracy= 0.93750\n",
      "Iter 24320, Minibatch Loss= 935.737915, Training Accuracy= 0.90625\n",
      "Iter 25600, Minibatch Loss= 1307.045898, Training Accuracy= 0.87500\n",
      "Iter 26880, Minibatch Loss= 1185.211426, Training Accuracy= 0.92969\n",
      "Iter 28160, Minibatch Loss= 1332.125732, Training Accuracy= 0.90625\n",
      "Iter 29440, Minibatch Loss= 1361.605469, Training Accuracy= 0.92969\n",
      "Iter 30720, Minibatch Loss= 438.367493, Training Accuracy= 0.90625\n",
      "Iter 32000, Minibatch Loss= 1028.990967, Training Accuracy= 0.93750\n",
      "Iter 33280, Minibatch Loss= 1115.346802, Training Accuracy= 0.92188\n",
      "Iter 34560, Minibatch Loss= 448.448792, Training Accuracy= 0.95312\n",
      "Iter 35840, Minibatch Loss= 339.147827, Training Accuracy= 0.93750\n",
      "Iter 37120, Minibatch Loss= 1174.727783, Training Accuracy= 0.90625\n",
      "Iter 38400, Minibatch Loss= 230.473572, Training Accuracy= 0.96094\n",
      "Iter 39680, Minibatch Loss= 144.154221, Training Accuracy= 0.96875\n",
      "Iter 40960, Minibatch Loss= 1684.666748, Training Accuracy= 0.92188\n",
      "Iter 42240, Minibatch Loss= 584.684204, Training Accuracy= 0.93750\n",
      "Iter 43520, Minibatch Loss= 292.644165, Training Accuracy= 0.96875\n",
      "Iter 44800, Minibatch Loss= 691.077148, Training Accuracy= 0.91406\n",
      "Iter 46080, Minibatch Loss= 234.877380, Training Accuracy= 0.98438\n",
      "Iter 47360, Minibatch Loss= 939.048340, Training Accuracy= 0.93750\n",
      "Iter 48640, Minibatch Loss= 1134.765503, Training Accuracy= 0.92969\n",
      "Iter 49920, Minibatch Loss= 841.006042, Training Accuracy= 0.89062\n",
      "Iter 51200, Minibatch Loss= 494.720093, Training Accuracy= 0.94531\n",
      "Iter 52480, Minibatch Loss= 519.220825, Training Accuracy= 0.92969\n",
      "Iter 53760, Minibatch Loss= 349.974640, Training Accuracy= 0.96094\n",
      "Iter 55040, Minibatch Loss= 645.214966, Training Accuracy= 0.94531\n",
      "Iter 56320, Minibatch Loss= 810.793274, Training Accuracy= 0.95312\n",
      "Iter 57600, Minibatch Loss= 987.639893, Training Accuracy= 0.89844\n",
      "Iter 58880, Minibatch Loss= 291.604980, Training Accuracy= 0.95312\n",
      "Iter 60160, Minibatch Loss= 522.503601, Training Accuracy= 0.92969\n",
      "Iter 61440, Minibatch Loss= 714.484863, Training Accuracy= 0.95312\n",
      "Iter 62720, Minibatch Loss= 613.155334, Training Accuracy= 0.94531\n",
      "Iter 64000, Minibatch Loss= 448.739929, Training Accuracy= 0.94531\n",
      "Iter 65280, Minibatch Loss= 893.877747, Training Accuracy= 0.92969\n",
      "Iter 66560, Minibatch Loss= 1002.027405, Training Accuracy= 0.92969\n",
      "Iter 67840, Minibatch Loss= 1322.309937, Training Accuracy= 0.93750\n",
      "Iter 69120, Minibatch Loss= 1183.021851, Training Accuracy= 0.94531\n",
      "Iter 70400, Minibatch Loss= 256.492401, Training Accuracy= 0.96094\n",
      "Iter 71680, Minibatch Loss= 722.165161, Training Accuracy= 0.93750\n",
      "Iter 72960, Minibatch Loss= 433.866760, Training Accuracy= 0.96094\n",
      "Iter 74240, Minibatch Loss= 310.220978, Training Accuracy= 0.94531\n",
      "Iter 75520, Minibatch Loss= 514.408508, Training Accuracy= 0.96094\n",
      "Iter 76800, Minibatch Loss= 540.333862, Training Accuracy= 0.97656\n",
      "Iter 78080, Minibatch Loss= 493.780396, Training Accuracy= 0.93750\n",
      "Iter 79360, Minibatch Loss= 183.647247, Training Accuracy= 0.97656\n",
      "Iter 80640, Minibatch Loss= 1867.674683, Training Accuracy= 0.90625\n",
      "Iter 81920, Minibatch Loss= 602.821472, Training Accuracy= 0.96094\n",
      "Iter 83200, Minibatch Loss= 630.452637, Training Accuracy= 0.94531\n",
      "Iter 84480, Minibatch Loss= 181.362274, Training Accuracy= 0.96875\n",
      "Iter 85760, Minibatch Loss= 71.654984, Training Accuracy= 0.96094\n",
      "Iter 87040, Minibatch Loss= 231.959641, Training Accuracy= 0.96094\n",
      "Iter 88320, Minibatch Loss= 672.573486, Training Accuracy= 0.92969\n",
      "Iter 89600, Minibatch Loss= 323.757019, Training Accuracy= 0.96875\n",
      "Iter 90880, Minibatch Loss= 242.680344, Training Accuracy= 0.96094\n",
      "Iter 92160, Minibatch Loss= 375.448242, Training Accuracy= 0.97656\n",
      "Iter 93440, Minibatch Loss= 368.056519, Training Accuracy= 0.95312\n",
      "Iter 94720, Minibatch Loss= 125.799408, Training Accuracy= 0.98438\n",
      "Iter 96000, Minibatch Loss= 175.216690, Training Accuracy= 0.96094\n",
      "Iter 97280, Minibatch Loss= 280.941223, Training Accuracy= 0.95312\n",
      "Iter 98560, Minibatch Loss= 388.665161, Training Accuracy= 0.95312\n",
      "Iter 99840, Minibatch Loss= 384.389801, Training Accuracy= 0.95312\n",
      "Iter 101120, Minibatch Loss= 284.231079, Training Accuracy= 0.96875\n",
      "Iter 102400, Minibatch Loss= 238.651230, Training Accuracy= 0.96094\n",
      "Iter 103680, Minibatch Loss= 187.117493, Training Accuracy= 0.96094\n",
      "Iter 104960, Minibatch Loss= 673.009644, Training Accuracy= 0.92969\n",
      "Iter 106240, Minibatch Loss= 507.738770, Training Accuracy= 0.96094\n",
      "Iter 107520, Minibatch Loss= 573.016052, Training Accuracy= 0.93750\n",
      "Iter 108800, Minibatch Loss= 143.668518, Training Accuracy= 0.97656\n",
      "Iter 110080, Minibatch Loss= 41.592041, Training Accuracy= 0.96875\n",
      "Iter 111360, Minibatch Loss= 198.190491, Training Accuracy= 0.96875\n",
      "Iter 112640, Minibatch Loss= 277.089111, Training Accuracy= 0.96094\n",
      "Iter 113920, Minibatch Loss= 361.236694, Training Accuracy= 0.93750\n",
      "Iter 115200, Minibatch Loss= 172.651230, Training Accuracy= 0.97656\n",
      "Iter 116480, Minibatch Loss= 508.136658, Training Accuracy= 0.95312\n",
      "Iter 117760, Minibatch Loss= 258.528748, Training Accuracy= 0.96875\n",
      "Iter 119040, Minibatch Loss= 606.713074, Training Accuracy= 0.96094\n",
      "Iter 120320, Minibatch Loss= 385.550568, Training Accuracy= 0.95312\n",
      "Iter 121600, Minibatch Loss= 217.502075, Training Accuracy= 0.96875\n",
      "Iter 122880, Minibatch Loss= 76.664566, Training Accuracy= 0.98438\n",
      "Iter 124160, Minibatch Loss= 195.854996, Training Accuracy= 0.96875\n",
      "Iter 125440, Minibatch Loss= 317.649048, Training Accuracy= 0.96094\n",
      "Iter 126720, Minibatch Loss= 482.577271, Training Accuracy= 0.96094\n",
      "Iter 128000, Minibatch Loss= 348.686218, Training Accuracy= 0.94531\n",
      "Iter 129280, Minibatch Loss= 178.059219, Training Accuracy= 0.94531\n",
      "Iter 130560, Minibatch Loss= 365.187042, Training Accuracy= 0.95312\n",
      "Iter 131840, Minibatch Loss= 320.178406, Training Accuracy= 0.96875\n",
      "Iter 133120, Minibatch Loss= 423.700195, Training Accuracy= 0.93750\n",
      "Iter 134400, Minibatch Loss= 845.734375, Training Accuracy= 0.93750\n",
      "Iter 135680, Minibatch Loss= 398.460632, Training Accuracy= 0.95312\n",
      "Iter 136960, Minibatch Loss= 428.739868, Training Accuracy= 0.95312\n",
      "Iter 138240, Minibatch Loss= 69.017815, Training Accuracy= 0.98438\n",
      "Iter 139520, Minibatch Loss= 191.892349, Training Accuracy= 0.96094\n",
      "Iter 140800, Minibatch Loss= 7.798531, Training Accuracy= 0.98438\n",
      "Iter 142080, Minibatch Loss= 451.196838, Training Accuracy= 0.94531\n",
      "Iter 143360, Minibatch Loss= 176.676620, Training Accuracy= 0.97656\n",
      "Iter 144640, Minibatch Loss= 65.769928, Training Accuracy= 0.96875\n",
      "Iter 145920, Minibatch Loss= 209.048645, Training Accuracy= 0.96094\n",
      "Iter 147200, Minibatch Loss= 64.718941, Training Accuracy= 0.97656\n",
      "Iter 148480, Minibatch Loss= 195.972809, Training Accuracy= 0.96094\n",
      "Iter 149760, Minibatch Loss= 309.028473, Training Accuracy= 0.93750\n",
      "Iter 151040, Minibatch Loss= 119.804871, Training Accuracy= 0.97656\n",
      "Iter 152320, Minibatch Loss= 243.429550, Training Accuracy= 0.96875\n",
      "Iter 153600, Minibatch Loss= 162.528107, Training Accuracy= 0.96875\n",
      "Iter 154880, Minibatch Loss= 235.143707, Training Accuracy= 0.97656\n",
      "Iter 156160, Minibatch Loss= 417.063904, Training Accuracy= 0.94531\n",
      "Iter 157440, Minibatch Loss= 21.922073, Training Accuracy= 0.98438\n",
      "Iter 158720, Minibatch Loss= 210.283142, Training Accuracy= 0.95312\n",
      "Iter 160000, Minibatch Loss= 448.902802, Training Accuracy= 0.95312\n",
      "Iter 161280, Minibatch Loss= 349.796082, Training Accuracy= 0.93750\n",
      "Iter 162560, Minibatch Loss= 141.425461, Training Accuracy= 0.96875\n",
      "Iter 163840, Minibatch Loss= 330.010254, Training Accuracy= 0.96094\n",
      "Iter 165120, Minibatch Loss= 237.412994, Training Accuracy= 0.96875\n",
      "Iter 166400, Minibatch Loss= 86.235855, Training Accuracy= 0.96094\n",
      "Iter 167680, Minibatch Loss= 225.672394, Training Accuracy= 0.96875\n",
      "Iter 168960, Minibatch Loss= 361.785126, Training Accuracy= 0.97656\n",
      "Iter 170240, Minibatch Loss= 233.521851, Training Accuracy= 0.95312\n",
      "Iter 171520, Minibatch Loss= 61.961311, Training Accuracy= 0.99219\n",
      "Iter 172800, Minibatch Loss= 140.391388, Training Accuracy= 0.97656\n",
      "Iter 174080, Minibatch Loss= 107.328117, Training Accuracy= 0.97656\n",
      "Iter 175360, Minibatch Loss= 78.400223, Training Accuracy= 0.97656\n",
      "Iter 176640, Minibatch Loss= 235.928391, Training Accuracy= 0.96094\n",
      "Iter 177920, Minibatch Loss= 53.897770, Training Accuracy= 0.98438\n",
      "Iter 179200, Minibatch Loss= 199.341797, Training Accuracy= 0.96875\n",
      "Iter 180480, Minibatch Loss= 29.097412, Training Accuracy= 0.98438\n",
      "Iter 181760, Minibatch Loss= 256.502441, Training Accuracy= 0.96094\n",
      "Iter 183040, Minibatch Loss= 177.186935, Training Accuracy= 0.95312\n",
      "Iter 184320, Minibatch Loss= 223.166687, Training Accuracy= 0.97656\n",
      "Iter 185600, Minibatch Loss= 15.879517, Training Accuracy= 0.97656\n",
      "Iter 186880, Minibatch Loss= 28.092850, Training Accuracy= 0.97656\n",
      "Iter 188160, Minibatch Loss= 24.160469, Training Accuracy= 0.98438\n",
      "Iter 189440, Minibatch Loss= 77.934593, Training Accuracy= 0.96875\n",
      "Iter 190720, Minibatch Loss= 195.965820, Training Accuracy= 0.96875\n",
      "Iter 192000, Minibatch Loss= 58.507057, Training Accuracy= 0.97656\n",
      "Iter 193280, Minibatch Loss= 61.528091, Training Accuracy= 0.96094\n",
      "Iter 194560, Minibatch Loss= 220.302338, Training Accuracy= 0.96875\n",
      "Iter 195840, Minibatch Loss= 57.704403, Training Accuracy= 0.96094\n",
      "Iter 197120, Minibatch Loss= 100.497955, Training Accuracy= 0.98438\n",
      "Iter 198400, Minibatch Loss= 208.470963, Training Accuracy= 0.97656\n",
      "Iter 199680, Minibatch Loss= 32.810532, Training Accuracy= 0.99219\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.976562\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y, keep_prob: dropout})\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x, y: batch_y, keep_prob: 1.})\n",
    "            print \"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \"{:.5f}\".format(acc)\n",
    "        step += 1\n",
    "    print \"Optimization Finished!\"\n",
    "\n",
    "    # Calculate accuracy for 256 mnist test images\n",
    "    print \"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: mnist.test.images[:256], y: mnist.test.labels[:256], keep_prob: 1.})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
